{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        (os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"root_path = '/kaggle/input/CORD-19-research-challenge/'\nmetadata_path = f'{root_path}/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nCount = 5000#1000#2168\nmeta_df = meta_df[meta_df[\"abstract\"].notna()]\nmeta_df = meta_df[meta_df[\"cord_uid\"].notna()]\nmeta_df = meta_df.head(Count)\nmeta_df[\"rank\"] = range(1, len(meta_df)+1)\nmeta_df.set_index(\"cord_uid\", inplace = True)\nmeta_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\nimport string\nimport pickle\n\ndef clean_text_round1(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.''' \n    text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \" \", text)# remove numbers\n    text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)#remove chinese and non ascii\n    return text\n\nround1 = lambda x: clean_text_round1(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the updated text\ndata_clean = pd.DataFrame(meta_df.abstract.apply(round1))\ndata_clean.head","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply a second round of cleaning\ndef clean_text_round2(text):\n    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n    text = re.sub('\\n', '', text)\n    return text\n\nround2 = lambda x: clean_text_round2(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at the updated text\ndata_clean = pd.DataFrame(data_clean.abstract.apply(round2))\ndata_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's pickle it for later use\nmeta_df.to_pickle(\"corpus.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words='english')\ndata_cv = cv.fit_transform(data_clean.abstract)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = data_clean.index\ndata_dtm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's pickle it for later use\ndata_dtm.to_pickle(\"dtm.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\ndata_clean.to_pickle('data_clean.pkl')\npickle.dump(cv, open(\"cv.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the document-term matrix\ndata = pd.read_pickle('dtm.pkl')\ndata = data.transpose()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the top 30 words in each paper abstract\ntop_dict = {}\nfor c in data.columns:\n    if isinstance(data[c], pd.DataFrame)==False:\n        top = data[c].sort_values(ascending=False).head(30)\n        top_dict[c]= list(zip(top.index, top.values))\n\ntop_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the top 15 words in each paper\nfor paper, top_words in top_dict.items():\n    print(paper)\n    print(', '.join([word for word, count in top_words[0:14]]))\n    print('---')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Look at the most common top words --> add them to the stop word list\nfrom collections import Counter\n\n# Let's first pull out the top 30 words for each paper\nwords = []\nfor paper in data.columns:\n    if isinstance(data[paper], pd.DataFrame)==False:\n        top = [word for (word, count) in top_dict[paper]]\n        for t in top:\n            words.append(t)\n        \nwords","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# If more than half of the papers have it as a top word, exclude it from the list\nadd_stop_words = [word for word, count in Counter(words).most_common(80) ]#if count > (Count//2)]\ncommon_word_count = Counter(words).most_common()\n#len(add_stop_words)\n#add_stop_words\nprint(common_word_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.rcdefaults()\nfig, ax = plt.subplots()#(figsize=(18,50))\ny_pos = [word for word, count in common_word_count]\ncount = [count for word, count in common_word_count]\nplt.barh(y_pos[:30], count[:30], align='center')\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Count of words')\nax.set_title('Common words bar plot')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's update our document-term matrix with the new list of stop words\nfrom sklearn.feature_extraction import text \nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Read in cleaned data\ndata_clean = pd.read_pickle('data_clean.pkl')\n\n# Add new stop words\nstop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n\n# Recreate document-term matrix\ncv = CountVectorizer(stop_words=stop_words)\ndata_cv = cv.fit_transform(data_clean.abstract)\ndata_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_stop.index = data_clean.index\n\n# Pickle it for later use\nimport pickle\npickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\ndata_stop.to_pickle(\"dtm_stop.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's make some word clouds!\nfrom wordcloud import WordCloud\n\nwc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reset the output dimensions\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 250]\n\ncord_uid = list(meta_df.index.values)\n\n# Create subplots for each paper\nfor index, paper in enumerate(data.columns):\n    wc.generate(data_clean.abstract[paper])\n    \n    plt.subplot(250, 4, index+1)\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(str(cord_uid[index]))\n    \nplt.show()\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's read in our document-term matrix\ndata = pd.read_pickle('dtm_stop.pkl')\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the necessary modules for LDA with gensim\nfrom gensim import matutils, models\nimport scipy.sparse\n\n# import logging\n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One of the required inputs is a term-document matrix\ntdm = data.transpose()\ntdm.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\nsparse_counts = scipy.sparse.csr_matrix(tdm)\ncorpus = matutils.Sparse2Corpus(sparse_counts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\ncv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\nid2word = dict((v, k) for k, v in cv.vocabulary_.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n# we need to specify two other parameters as well - the number of topics and the number of passes\nlda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10, passes=10)\nlda.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LDA for num_topics = 3\nlda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=15, passes=20)\nlda.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LDA for num_topics = 4\nlda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=20, passes=50)\nlda.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a function to pull out nouns from a string of text\nfrom nltk import word_tokenize, pos_tag\n\ndef nouns(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n    is_noun = lambda pos: pos[:2] == 'NN'\n    tokenized = word_tokenize(text)\n    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n    return ' '.join(all_nouns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Read in the cleaned data, before the CountVectorizer step\ndata_clean = pd.read_pickle('data_clean.pkl')\ndata_clean","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the nouns function to the transcripts to filter only on nouns\ndata_nouns = pd.DataFrame(data_clean.abstract.apply(nouns))\ndata_nouns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new document-term matrix using only nouns\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Re-add the additional stop words since we are recreating the document-term matrix\nadd_stop_words = [\"background\", \"conclusions\", \"results\", \"mehod\", \"methods\", \"objective\", \"objectives\", \"hypothesis\", \n                  \"findings\", \"significance\", \"methodology\", \"study\", \"design\", \"discussion\", \"electronic\", \"supplementary\",\n                  \"material\", \"purpose\", \"rationle\"]\nstop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n\n# Recreate a document-term matrix with only nouns\ncvn = CountVectorizer(stop_words=stop_words)\ndata_cvn = cvn.fit_transform(data_nouns.abstract)\ndata_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\ndata_dtmn.index = data_nouns.index\ndata_dtmn","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the gensim corpus\ncorpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n\n# Create the vocabulary dictionary\nid2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's start with 10 topics\nldan = models.LdaModel(corpus=corpusn, num_topics=10, id2word=id2wordn, passes=10)\nldan.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try topics = 15\nldan = models.LdaModel(corpus=corpusn, num_topics=15, id2word=id2wordn, passes=20)\nldan.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try 20 topics\nldan = models.LdaModel(corpus=corpusn, num_topics=20, id2word=id2wordn, passes=50)\nldan.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's create a function to pull out nouns from a string of text\ndef nouns_adj(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n    tokenized = word_tokenize(text)\n    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n    return ' '.join(nouns_adj)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the nouns function to the abstract to filter only on nouns\ndata_nouns_adj = pd.DataFrame(data_clean.abstract.apply(nouns_adj))\ndata_nouns_adj","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\ncvna = CountVectorizer(stop_words=stop_words, max_df=.8)\ndata_cvna = cvna.fit_transform(data_nouns_adj.abstract)\ndata_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\ndata_dtmna.index = data_nouns_adj.index\ndata_dtmna","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the gensim corpus\ncorpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n\n# Create the vocabulary dictionary\nid2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's start with 10 topics\nldana = models.LdaModel(corpus=corpusna, num_topics=10, id2word=id2wordna, passes=10)\nldana.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try 15 topics\nldana = models.LdaModel(corpus=corpusna, num_topics=15, id2word=id2wordna, passes=20)\nldana.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's try 20 topics\nldana = models.LdaModel(corpus=corpusna, num_topics=20, id2word=id2wordna, passes=50)\nldana.print_topics()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Our final LDA model (for now)\nldana = models.LdaModel(corpus=corpusna, num_topics=20, id2word=id2wordna, passes=80)\nldana.print_topics(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's take a look at which topics each transcript contains\ncorpus_transformed = ldana[corpusna]\nt_lst = []\nfor tup_lst in corpus_transformed:\n    t = list(map(lambda x: x[0], tup_lst))\n    t_lst.append(t)\npaper_topic = list(zip([a for a in t_lst], data_dtmna.index))\nwith open('paper_topic.txt', 'w') as f:\n    for item in paper_topic:\n        f.write(\"%s\\n\" % str(item))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}